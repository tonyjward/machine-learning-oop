{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Particle Swarm Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will walk through how to use [Particle Swarm Optimisation](https://en.wikipedia.org/wiki/Particle_swarm_optimization)  to produce optimal Linear Regression models for a range of custom loss metrics. Using a methodology introduced by [Dietterich (1998)](https://sci2s.ugr.es/keel/pdf/algorithm/articulo/dietterich1998.pdf) we will perform a siginificance test on the holdout predictions, to see whether training on custom loss metrics produces significantly better models.\n",
    "\n",
    "## Model Error\n",
    "\n",
    "In regression problems the target variable is continuous and our model can make errors in two ways\n",
    "1. Underpredict the target\n",
    "2. Overpredict the target\n",
    "\n",
    "The Ordinary Least Squares and Gradient Descent implementations of Linear Regression both minmise Mean Squared Error as the loss function. Mean Square error places equal importance on an underprediction vs an overprediction. However in practice our preferences for both types of error are rarely the same instead they are driven by the business problem at hand. For example to improve efficiency a utility company might want to know how much electricity they can put through a [transformer](https://en.wikipedia.org/wiki/Transformer) without it overheating. For this use case an overprediction would mean a loss of efficiency, but an underprediction could lead to a failure of the transformer, blackouts for customers and heavy regulatory fines.\n",
    "\n",
    "One solution to this problem is to add on \"safety\" margins to our predictions. However a more elegant approach is to encode our preferences directly in a function which we use directly to optimise our model coefficients.\n",
    "\n",
    "## Particle Swarm Optimisation\n",
    "Particle swarm optimization (PSO) is a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. It solves a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formulae over the particle's position and velocity. Each particle's movement is influenced by its local best known position, but is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. This is expected to move the swarm toward the best solutions.\n",
    "\n",
    "I have [coded up an implementation](https://github.com/tonyjward/machine-learning-oop/blob/master/twlearn/ParticleSwarm.py) of PSO based on the above Wiki page which i'll use to produce throughout this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling\n",
    "from twlearn import LinearRegression\n",
    "\n",
    "# Evaluation of the model\n",
    "import sklearn.model_selection\n",
    "from twlearn.metrics import Rmse, Mae, five_by_two_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "For this notebook we will work with the [Boston Housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) available as part of scikit learn. The objective is to predict the Median value of owner-occupied homes by training a model on past data. This is a supervised machine learning regression task: given past data we want to train a model to predict a continous outcome on testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "X = np.array(boston.data)\n",
    "Y = np.array(boston.target)\n",
    "\n",
    "# split the data into training and testing\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.33, random_state = 1)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Sqaures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets train two linear regression models \n",
    "1. Using ordinary least squares\n",
    "2. Using particle swarm optimisation with MSE loss metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS\n",
    "OLS_model = LinearRegression()\n",
    "OLS_model.fit(X_train, Y_train, optimiser = 'OLS')\n",
    "\n",
    "# Particle Swarm Regression - Rmse\n",
    "PSO_RMSE_model = LinearRegression()\n",
    "PSO_RMSE_model.fit(X_train, Y_train, optimiser = 'PSO', loss = Rmse)\n",
    "\n",
    "OLS_coef = OLS_model.coefficients()['coefficients']\n",
    "PSO_RMSE_coef = PSO_RMSE_model.coefficients()['coefficients']\n",
    "\n",
    "print(np.c_[OLS_coef, PSO_RMSE_coef])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close but not exact. Increasing the number of iterations and particles from their defaults of 500 and 300 respectively gets us much close to the OLS coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSO_RMSE_model.fit(X_train, Y_train, optimiser = 'PSO', loss = Rmse, num_iterations = 5000, no_particles = 500)\n",
    "PSO_RMSE_coef = PSO_RMSE_model.coefficients()['coefficients']\n",
    "print(np.c_[OLS_coef, PSO_RMSE_coef])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok lets try optimising a different error metric - mean absolute error, using the num_iterations and no_particles from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSO_MAE_model = LinearRegression()\n",
    "PSO_MAE_model.fit(X_train, Y_train, optimiser = 'PSO', loss = Mae, num_iterations = 5000, no_particles = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now compare the predictions to the original OLS model using a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, Y_test, loss):\n",
    "    \"\"\" Take a model and and some test data and produce test metrics\n",
    "    Arguments:\n",
    "        model         -- a fitted model\n",
    "        X_test        -- test data - a numpy array\n",
    "        loss_function -- a loss function to assess predictions\n",
    "\n",
    "    Returns:\n",
    "        loss - calculated loss\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X_test)\n",
    "    return loss(predictions, Y_test)\n",
    "\n",
    "print(f\"OLS MAE: {evaluate(OLS_model, X_test, Y_test, Mae)}\")\n",
    "print(f\"PSO MAE: {evaluate(PSO_MAE_model, X_test, Y_test, Mae)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! The particle swarm regression model optimised for MAE performs better than the Ordinary Least Square model compared using MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is this significant?\n",
    "519 data points is not very much. What could we do to assess the significance. Our first thought might be to do 10-fold cross validation, and compare the performance between both algorithms on each fold using a students paired t test. However this would be incorrect, since one of the core assumptions (independence) of the t test would be violated. A good overview of our options is provided here, and we elect for the 5 by 2 cross validation statistic.\n",
    "\n",
    "The intuition behind this approach is that we use 50% of the data for training and 50% for testing ONCE to get our base estimate for the treatment effect (difference between models). Then we do 5 repeats of 2-fold cross validation to get an estimate for the variance in the treatment effect, which we use to calculate a t-statistic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
