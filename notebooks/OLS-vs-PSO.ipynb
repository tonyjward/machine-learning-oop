{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Particle Swarm Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will walk through how to use [Particle Swarm Optimisation](https://en.wikipedia.org/wiki/Particle_swarm_optimization)  to produce optimal Linear Regression models for a range of custom loss metrics. Using a methodology introduced by [Dietterich (1998)](https://sci2s.ugr.es/keel/pdf/algorithm/articulo/dietterich1998.pdf) we will perform a siginificance test on the holdout predictions, to see whether training on custom loss metrics produces significantly better models.\n",
    "\n",
    "## Model Error\n",
    "\n",
    "In regression problems the target variable is continuous and our model can make errors in two ways\n",
    "1. Underpredict the target\n",
    "2. Overpredict the target\n",
    "\n",
    "The Ordinary Least Squares and Gradient Descent implementations of Linear Regression both minmise Mean Squared Error as the loss function. Mean Square error places equal importance on an underprediction vs an overprediction. However in practice our preferences for both types of error are rarely the same instead they are driven by the business problem at hand. For example to improve efficiency a utility company might want to know how much electricity they can put through a [transformer](https://en.wikipedia.org/wiki/Transformer) without it overheating. For this use case an overprediction would mean a loss of efficiency, but an underprediction could lead to a failure of the transformer, blackouts for customers and heavy regulatory fines.\n",
    "\n",
    "One solution to this problem is to add on \"safety\" margins to our predictions. However a more elegant approach is to encode our preferences directly in a function which we use directly to optimise our model coefficients.\n",
    "\n",
    "## Particle Swarm Optimisation\n",
    "Particle swarm optimization (PSO) is a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. It solves a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formulae over the particle's position and velocity. Each particle's movement is influenced by its local best known position, but is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. This is expected to move the swarm toward the best solutions.\n",
    "\n",
    "I have [coded up an implementation](https://github.com/tonyjward/machine-learning-oop/blob/master/twlearn/ParticleSwarm.py) of PSO based on the above Wiki page which i'll use to produce throughout this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling\n",
    "from twlearn import LinearRegression\n",
    "\n",
    "# Evaluation of the model\n",
    "import sklearn.model_selection\n",
    "from twlearn.metrics import Rmse\n",
    "NO_FOLDS = 2\n",
    "NO_REPEATS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "For this notebook we will work with the [Boston Housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) available as part of scikit learn. The objective is to predict the Median value of owner-occupied homes by training a model on past data. This is a supervised machine learning regression task: given past data we want to train a model to predict a continous outcome on testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "X = np.array(boston.data)\n",
    "Y = np.array(boston.target)\n",
    "\n",
    "# split the data into training and testing\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.33, random_state = 1)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Sqaures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets train two linear regression models \n",
    "1. Using ordinary least squares\n",
    "2. Using particle swarm optimisation with MSE loss metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS\n",
    "OLS_model = LinearRegression()\n",
    "OLS_model.fit(X_train, Y_train, optimiser = 'OLS')\n",
    "\n",
    "# Particle Swarm Regression - Rmse\n",
    "PSO_RMSE_model = LinearRegression()\n",
    "PSO_RMSE_model.fit(X_train, Y_train, optimiser = 'PSO', loss = Rmse)\n",
    "\n",
    "OLS_coef = OLS_model.coefficients()['coefficients']\n",
    "PSO_RMSE_coef = PSO_RMSE_model.coefficients()['coefficients']\n",
    "\n",
    "print(np.c_[OLS_coef, PSO_RMSE_coef])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close but not exact. Increasing the number of iterations and particles from their defaults of 500 and 300 respectively gets us much close to the OLS coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSO_RMSE_model.fit(X_train, Y_train, optimiser = 'PSO', loss = Rmse, num_iterations = 5000, no_particles = 500)\n",
    "PSO_RMSE_coef = PSO_RMSE_model.coefficients()['coefficients']\n",
    "print(np.c_[OLS_coef, PSO_RMSE_coef])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok lets try optimising a different error metric - mean absolute error, using the num_iterations and no_particles from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mae(predictions, actual):\n",
    "    \"\"\"\n",
    "    Calculate Mean Absolute Error\n",
    "\n",
    "    Arguments:\n",
    "        predictions: predictions numpy array of size (no_examples, no_solutions)\n",
    "        actuals: 1D numpy array of size (no_examples, 1)\n",
    "\n",
    "    Returns:\n",
    "        mae: Mean absolute Error for each solution - numpy array of size (1, no_solutions)\n",
    "    \n",
    "    Approach:\n",
    "    predictions can be a 1d array which corresponds to one set of model predictions OR\n",
    "    it can be a matrix of predictions, where each column represents a set of predictions\n",
    "    for a specific model. \n",
    "    \"\"\"\n",
    "    assert(predictions.shape[0] == actual.shape[0])\n",
    "    if len(predictions.shape) == 1:\n",
    "        predictions = predictions.reshape(-1, 1)\n",
    "    if len(actual.shape) == 1:\n",
    "        actual = actual.reshape(-1, 1)\n",
    "\n",
    "    absolute_errors = np.abs(predictions - actual)\n",
    "    assert(absolute_errors.shape == predictions.shape)\n",
    "\n",
    "    return np.mean(absolute_errors, axis = 0, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class Test_1D_solution(unittest.TestCase):\n",
    "    \"\"\" Test when predictions argument contains one set of solutions\"\"\"\n",
    "    def setUp(self):\n",
    "        self.predictions = np.array([1, 2, 3])\n",
    "        self.actuals = np.array([0.9, 2.2, 2.7])\n",
    "\n",
    "    def test_Mae(self):\n",
    "        mae = Mae(self.predictions, self.actuals)\n",
    "        self.assertIsNone(np.testing.assert_allclose(mae, 0.2))\n",
    "        \n",
    "class Test_2D_solutions(unittest.TestCase):\n",
    "    \"\"\" Test when predictions argument contains two set of solutions\"\"\"\n",
    "    def setUp(self):\n",
    "        self.predictions = np.array([[1,1], [2, 2], [3, 3]])\n",
    "        self.actuals = np.array([0.9, 2.2, 2.7])\n",
    "\n",
    "    def test_Mae(self):\n",
    "        mae = Mae(self.predictions, self.actuals)\n",
    "        self.assertIsNone(np.testing.assert_allclose(mae, np.array([[0.2, 0.2]])))\n",
    "        \n",
    "# run tests\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSO_MAE_model = LinearRegression()\n",
    "PSO_MAE_model.fit(X_train, Y_train, optimiser = 'PSO', loss = Mae, num_iterations = 5000, no_particles = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now compare the predictions to the original OLS model using a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, Y_test, loss):\n",
    "    \"\"\" Take a model and and some test data and produce test metrics\n",
    "    Arguments:\n",
    "        model         -- a fitted model\n",
    "        X_test        -- test data - a numpy array\n",
    "        loss_function -- a loss function to assess predictions\n",
    "\n",
    "    Returns:\n",
    "        loss - calculated loss\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X_test)\n",
    "    return loss(predictions, Y_test)\n",
    "\n",
    "print(f\"OLS MAE: {evaluate(OLS_model, X_test, Y_test, Mae)}\")\n",
    "print(f\"PSO MAE: {evaluate(PSO_MAE_model, X_test, Y_test, Mae)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! The particle swarm regression model optimised for MAE performs better than the Ordinary Least Square model compared using MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is this significant?\n",
    "519 data points is not very much. What could we do to assess the significance. Our first thought might be to do 10-fold cross validation, and compare the performance between both algorithms on each fold using a students paired t test. However this would be incorrect, since one of the core assumptions (independence) of the t test would be violated. A good overview of our options is provided here, and we elect for the 5 by 2 cross validation statistic.\n",
    "\n",
    "The intuition behind this approach is that we use 50% of the data for training and 50% for testing ONCE to get our base estimate for the treatment effect (difference between models). Then we do 5 repeats of 2-fold cross validation to get an estimate for the variance in the treatment effect, which we use to calculate a t-statistic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(no_repeats, no_folds, loss, kwargs):\n",
    "    \"\"\"\n",
    "    Approach:\n",
    "        We want the random numbers for repeat 1 to be different for repeat 2, etc\n",
    "        however we also want the random numbers for repeat 1, to be the same\n",
    "        each time we run the cross_validation to ensure we can compare algorithm\n",
    "        performance. We therefore set the seed to be the repeat number.\n",
    "    \"\"\"  \n",
    "    boston = load_boston()\n",
    "\n",
    "    X = np.array(boston.data)\n",
    "    Y = np.array(boston.target)\n",
    "\n",
    "    no_examples, no_features = X.shape\n",
    "\n",
    "    cv_results = {}\n",
    "\n",
    "    for repeat in range(NO_REPEATS):\n",
    "        print(f\"repeat: {repeat}\")\n",
    "        \n",
    "        np.random.seed(repeat)\n",
    "        folds = np.random.randint(low = 0, high = NO_FOLDS , size = no_examples)\n",
    "        \n",
    "        cv_results[repeat] = {}\n",
    "  \n",
    "        for fold in range(NO_FOLDS):\n",
    "            print(f\"fold: {fold}\")\n",
    "\n",
    "            X_train = X[folds != fold,:]\n",
    "            X_test  = X[folds == fold,:]\n",
    "            Y_train = Y[folds != fold]\n",
    "            Y_test  = Y[folds == fold]           \n",
    "\n",
    "            # train model\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, Y_train, **kwargs)\n",
    "\n",
    "            # evaluate model\n",
    "            cv_results[repeat][fold] = evaluate(model, X_test, Y_test, loss)\n",
    "\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_LOSS = Mae\n",
    "\n",
    "print(\"Training OLS Model\")\n",
    "ols_results = cross_validate(no_repeats = NO_REPEATS, no_folds = NO_FOLDS, loss = CUSTOM_LOSS, \n",
    "                             kwargs = {\"optimiser\":\"OLS\"})\n",
    "\n",
    "print(\"Training PSO Model\")\n",
    "pso_results = cross_validate(no_repeats = NO_REPEATS, no_folds = NO_FOLDS, loss = CUSTOM_LOSS,\n",
    "                             kwargs = {\"optimiser\":'PSO', \"loss\":CUSTOM_LOSS, \"num_iterations\":5000, \"no_particles\":500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparing Models using Mean Absolute Error\")\n",
    "for repeat in range(NO_REPEATS):\n",
    "    for fold in range(NO_FOLDS):\n",
    "        print(f\"Error for repeat {repeat} fold {fold}: OLS: {np.round(ols_results[repeat][fold],2)} PSO: {np.round(pso_results[repeat][fold],2)} difference : {np.round(ols_results[repeat][fold] - pso_results[repeat][fold],2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twlearn.metrics import five_by_two_cv\n",
    "from scipy import stats\n",
    "\n",
    "t_statistic, average_differences = five_by_two_cv(ols_results, pso_results)\n",
    "p_value = (1 - stats.t.cdf(t_statistic, df=5)) * 2\n",
    "\n",
    "print(f\"The t statistic is {np.round(t_statistic,2)} which has a p value of {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the distribution of error vs actual values. What if we really didn't want to underpredict at the high actual values? <insert image>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourth_quadrant(predictions, actual, multiplier = 100):\n",
    "    \"\"\"\n",
    "    Calculate Mean Absolute Error\n",
    "\n",
    "    Arguments:\n",
    "        predictions: predictions numpy array of size (no_examples, no_particles)\n",
    "        actuals: 1D numpy array of size (no_examples, 1)\n",
    "        multiplier: int - how much weight to give underpredictions for positive actual values\n",
    "\n",
    "    Returns:\n",
    "        mae: mae for each particle - numpy array of size (1, no_particles)\n",
    "    \"\"\"\n",
    "    assert(predictions.shape[0] == actual.shape[0])\n",
    "\n",
    "    if len(predictions.shape) == 1:\n",
    "        predictions = predictions.reshape(-1, 1)\n",
    "    if len(actual.shape) == 1:\n",
    "        actual = actual.reshape(-1, 1)\n",
    "     \n",
    "    errors = predictions - actual\n",
    "    assert(errors.shape == predictions.shape)\n",
    " \n",
    "    negative_error_index = errors < 0\n",
    "    positive_actual_index = actual > 0\n",
    "\n",
    "    extra_weight_index = np.logical_and(negative_error_index, positive_actual_index)\n",
    "\n",
    "    # cautious adjustment\n",
    "    adjustment = np.ones(shape = errors.shape)\n",
    "    adjustment[extra_weight_index] = multiplier\n",
    "\n",
    "    # squared error with adjustment\n",
    "    adjusted_squared_error = np.multiply(np.square(errors), adjustment)\n",
    "\n",
    "    return np.mean(adjusted_squared_error, axis = 0, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test or show example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_LOSS = fourth_quadrant\n",
    "\n",
    "print(\"Training OLS Model\")\n",
    "ols_results = cross_validate(no_repeats = NO_REPEATS, no_folds = NO_FOLDS, loss = CUSTOM_LOSS, \n",
    "                             kwargs = {\"optimiser\":\"OLS\"})\n",
    "\n",
    "print(\"Training PSO Model\")\n",
    "pso_results = cross_validate(no_repeats = NO_REPEATS, no_folds = NO_FOLDS, loss = CUSTOM_LOSS,\n",
    "                             kwargs = {\"optimiser\":'PSO', \"loss\":CUSTOM_LOSS, \"num_iterations\":5000, \"no_particles\":500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparing Models using Fourth Quadrant Error\")\n",
    "for repeat in range(NO_REPEATS):\n",
    "    for fold in range(NO_FOLDS):\n",
    "        print(f\"Error for repeat {repeat} fold {fold}: OLS: {np.round(ols_results[repeat][fold],2)} PSO: {np.round(pso_results[repeat][fold],2)} difference : {np.round(ols_results[repeat][fold] - pso_results[repeat][fold],2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, average_differences = five_by_two_cv(ols_results, pso_results)\n",
    "p_value = (1 - stats.t.cdf(t_statistic, df=5)) * 2\n",
    "\n",
    "print(f\"The t statistic is {np.round(t_statistic,2)} which has a p value of {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  N.B. We did not consider a one sided P value here because we could not be absolutely certain that the rats would all benefit from a high protein diet in comparison with those on a low protein diet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
