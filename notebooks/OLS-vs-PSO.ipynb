{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Particle Swarm Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will walk through how to use [Particle Swarm Optimisation](https://en.wikipedia.org/wiki/Particle_swarm_optimization)  to produce optimal Linear Regression models for a range of custom loss metrics. Using a methodology introduced by [Dietterich (1998)](https://sci2s.ugr.es/keel/pdf/algorithm/articulo/dietterich1998.pdf) we will perform a siginificance test on the holdout predictions, to see whether training on custom loss metrics produces significantly better models.\n",
    "\n",
    "## Model Error\n",
    "\n",
    "In regression problems the target variable is continuous and our model can make errors in two ways\n",
    "1. Underpredict the target\n",
    "2. Overpredict the target\n",
    "\n",
    "The Ordinary Least Squares and Gradient Descent implementations of Linear Regression both minmise Mean Squared Error as the loss function. Mean Square error places equal importance on an underprediction vs an overprediction. However in practice our preferences for both types of error are rarely the same instead they are driven by the business problem at hand. For example to improve efficiency a utility company might want to know how much electricity they can put through a [transformer](https://en.wikipedia.org/wiki/Transformer) without it overheating. For this use case an overprediction would mean a loss of efficiency, but an underprediction could lead to a failure of the transformer, blackouts for customers and heavy regulatory fines.\n",
    "\n",
    "One solution to this problem is to add on \"safety\" margins to our predictions. However a more elegant approach is to encode our preferences directly in a function which we use directly to optimise our model coefficients.\n",
    "\n",
    "## Particle Swarm Optimisation\n",
    "Particle swarm optimization (PSO) is a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. It solves a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formulae over the particle's position and velocity. Each particle's movement is influenced by its local best known position, but is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. This is expected to move the swarm toward the best solutions.\n",
    "\n",
    "I have [coded up an implementation](https://github.com/tonyjward/machine-learning-oop/blob/master/twlearn/ParticleSwarm.py) of PSO based on the above Wiki page which i'll use to produce throughout this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modelling\n",
    "from twlearn import LinearRegression\n",
    "\n",
    "# Evaluation of the model\n",
    "import sklearn.model_selection\n",
    "NO_FOLDS = 2\n",
    "NO_REPEATS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "For this notebook we will work with the [Boston Housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) available as part of scikit learn. The objective is to predict the Median value of owner-occupied homes by training a model on past data. This is a supervised machine learning regression task: given past data we want to train a model to predict a continous outcome on testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "X = np.array(boston.data)\n",
    "Y = np.array(boston.target)\n",
    "\n",
    "# split the data into training and testing\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.33, random_state = 1)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets train an ordinary least squares regression model and examine a residuals (defined as actual - predicted) vs predicted plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# train model\n",
    "OLS_model = LinearRegression()\n",
    "OLS_model.fit(X_train, Y_train, optimiser = 'OLS')\n",
    "\n",
    "# examine predicted vs residuals\n",
    "def residual_scatter_plot(predicted, actual):\n",
    "    if len(predicted.shape) == 1:\n",
    "        predicted = predicted.reshape(-1,1)\n",
    "    if len(actual.shape) == 1:\n",
    "        actual = actual.reshape(-1, 1)\n",
    "    \n",
    "    error = actual - predicted\n",
    "    \n",
    "    plt.scatter(predicted, error)\n",
    "    plt.axhline(color='r')\n",
    "    \n",
    "residual_scatter_plot(predicted = OLS_model.predict(X_train),\n",
    "                      actual    = Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a definite upside down U patturn in the residuals. This indicates that the relationship between the input variables and the target is not linear, which is a required assumption of Linear Regression. Lets instead take logs of the target variable and re-run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform target\n",
    "log_Y_train = np.log(Y_train)\n",
    "\n",
    "# Train model\n",
    "log_OLS_model = LinearRegression()\n",
    "log_OLS_model.fit(X_train, \n",
    "                  log_Y_train, \n",
    "                  optimiser = 'OLS')\n",
    "\n",
    "# Examine predicted vs residuals\n",
    "residual_scatter_plot(predicted = log_OLS_model.predict(X_train),\n",
    "                      actual    = log_Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That has removed the U shaped pattern in the residuals. If we look at the spread of residuals from left to right the variance appears to reduce which means we can't assume Homoscedasticity of errors. This invalidates any siginificance tests done on the resulting model coefficients. However we won't be performing variable selection in this notebook. Rather we will be focusing on the underlying loss functions and optimisation procedures, therefore I'm happy to proceed without worrying about this.\n",
    "\n",
    "## Particle Swarm Regression\n",
    "\n",
    "Before we can perform particle swarm regression we need to construct a loss function. Details of the required input and output dimensions are provided in the doc string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rmse(predictions, actual):\n",
    "    \"\"\"\n",
    "   Calculate Root mean square error\n",
    "\n",
    "    Arguments:\n",
    "        predictions: predictions numpy array of size (no_examples, no_solutions)\n",
    "        actuals: 1D numpy array of size (no_examples, 1)\n",
    "\n",
    "    Returns:\n",
    "        rmse: rmse for each solution - numpy array of size (1, no_solutions)\n",
    "    \n",
    "    Approach:\n",
    "    predictions can be a 1d array which corresponds to one set of model predictions OR\n",
    "    if can be a matrix of predictions, where each column represents a set of predictions\n",
    "    for a specific model (which is usefule for particle swarm optimisation)\n",
    "    \"\"\"\n",
    "    assert(predictions.shape[0] == actual.shape[0])\n",
    "    if len(predictions.shape) == 1:\n",
    "        predictions = predictions.reshape(-1, 1)\n",
    "    if len(actual.shape) == 1:\n",
    "        actual = actual.reshape(-1, 1)\n",
    "   \n",
    "    no_examples, no_solutions = predictions.shape\n",
    "\n",
    "    squared_error = np.square(predictions - actual)\n",
    "    assert(squared_error.shape == predictions.shape)\n",
    "\n",
    "    sum_of_squares = np.sum(squared_error, axis = 0, keepdims = True)\n",
    "    assert(sum_of_squares.shape == (1, no_solutions))\n",
    "    \n",
    "    rmse = np.sqrt(sum_of_squares)\n",
    "    assert(rmse.shape == (1, no_solutions))\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding we must test out function works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class Test_1_solution(unittest.TestCase):\n",
    "    \"\"\" Test covers predictions argument containing one set of solutions\"\"\"\n",
    "    def setUp(self):\n",
    "        self.predictions = np.array([1, 2, 3])\n",
    "        self.actuals = np.array([0.9, 2.2, 2.7])\n",
    "\n",
    "    def test_Rmse(self):\n",
    "        rmse = Rmse(self.predictions, self.actuals)\n",
    "        self.assertIsNone(np.testing.assert_allclose(rmse, 0.374165739))\n",
    "\n",
    "class Test_2_solutions(unittest.TestCase):\n",
    "    \"\"\" Test covers predictions argument containing two set of solutions\"\"\"\n",
    "    def setUp(self):\n",
    "        self.predictions = np.array([[1,1], [2, 2], [3, 3]])\n",
    "        self.actuals = np.array([0.9, 2.2, 2.7])\n",
    "\n",
    "    def test_Rmse(self):\n",
    "        rmse = Rmse(self.predictions, self.actuals)\n",
    "        self.assertIsNone(np.testing.assert_allclose(rmse, np.array([[0.374165739, 0.374165739]])))\n",
    "        \n",
    "# run tests\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets train the model using our Rmse loss function, and compare the model coeffecients to the ordinary least square implementation. For brevity i'm going to retrain the OLS model and remove the log prefix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Ordinary Least Squares\n",
    "OLS = LinearRegression()\n",
    "OLS.fit(X_train, \n",
    "        log_Y_train, \n",
    "        optimiser = 'OLS')\n",
    "\n",
    "# Train Particle Swarm Regression using Rmse\n",
    "PSO_Rmse = LinearRegression()\n",
    "PSO_Rmse.fit(X_train, log_Y_train, optimiser = 'PSO', loss = Rmse)\n",
    "\n",
    "# Compare model coefficients\n",
    "print(\"Compare model coefficients\")\n",
    "OLS_coef = OLS.coefficients()['coefficients']\n",
    "PSO_Rmse_coef = PSO_Rmse.coefficients()['coefficients']\n",
    "print(np.c_[OLS_coef, PSO_Rmse_coef])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model coefficients are close but not exactly the same. Ordinary Least Squares finds the coefficients using a closed form solution we know that is must give the model coefficients that minimise RMSE on the training data. Lets double check that is the case using a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, Y, loss):\n",
    "    \"\"\" Take a model and and some test data and produce test metrics\n",
    "    Arguments:\n",
    "        model         -- a fitted model\n",
    "        X_test        -- test data - a numpy array\n",
    "        loss_function -- a loss function to assess predictions\n",
    "\n",
    "    Returns:\n",
    "        loss - calculated loss\n",
    "    \"\"\"\n",
    "    predictions = model.predict(X)\n",
    "    return loss(predictions, Y)\n",
    "\n",
    "# compare models using Rmse\n",
    "print(f\"OLS has a training Rmse of: {evaluate(OLS, X_train, log_Y_train, Rmse)}\")\n",
    "print(f\"PSO has a training Rmse of: {evaluate(PSO_Rmse, X_train, log_Y_train, Rmse)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets increase the number of iterations and particles from their defaults of 500 and 300 respectively to see if we can get closer to the \"true\" coefficient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Particle Swarm Regression using Rmse and refined hyperparameters\n",
    "PSO_Rmse = LinearRegression()\n",
    "PSO_Rmse.fit(X_train, log_Y_train, optimiser = 'PSO', loss = Rmse, num_iterations = 5000, no_particles = 500)\n",
    "\n",
    "# Compare model coefficients\n",
    "print(\"Compare model coefficients\")\n",
    "PSO_Rmse_coef = PSO_Rmse.coefficients()['coefficients']\n",
    "print(np.c_[OLS_coef, PSO_Rmse_coef])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Compare model training error using Rmse\n",
    "print(f\"OLS has a training Rmse of: {evaluate(OLS, X_train, log_Y_train, Rmse)}\")\n",
    "print(f\"PSO has a training Rmse of: {evaluate(PSO_Rmse, X_train, log_Y_train, Rmse)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising Mean Absolute Error \n",
    "Now we have established that particle swarm can succssfully optimise Rmse lets try Mean Absolute Error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twlearn.metrics import Mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Particle Swarm Regression using Mae and refined hyperparameters\n",
    "PSO_Mae = LinearRegression()\n",
    "PSO_Mae.fit(X_train, log_Y_train, optimiser = 'PSO', loss = Mae, num_iterations = 5000, no_particles = 500)\n",
    "\n",
    "# Compare model training error using Mae\n",
    "print(f\"OLS has a training Mae of: {evaluate(OLS, X_train, log_Y_train, Mae)}\")\n",
    "print(f\"PSO has a training Mae of: {evaluate(PSO_Mae, X_train, log_Y_train, Mae)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great so we have established that we can minimise a custom loss function on the training data. However what we really care about is the generalisation error of the model. Lets see how both models perform on the test set with respect to Mae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model test error using Mae\n",
    "log_Y_test = np.log(Y_test)\n",
    "print(f\"OLS has a test Mae of: {evaluate(OLS, X_test, log_Y_test, Mae)}\")\n",
    "print(f\"PSO has a test Mae of: {evaluate(PSO_Mae, X_test, log_Y_test, Mae)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! As we had hoped the particle swarm model trained on Mae performs better than the Ordinary Least square model on test data when evaluated using Mae. For curiosity lets just see how well the PSO_Mae model performs with respect to Rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model test error using Rmse\n",
    "log_Y_test = np.log(Y_test)\n",
    "print(f\"OLS has a test Rmse of: {evaluate(OLS, X_test, log_Y_test, Rmse)}\")\n",
    "print(f\"PSO has a test Rmse of: {evaluate(PSO_Mae, X_test, log_Y_test, Rmse)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the OLS model outperforms the Pso_Mae model which we would expect, unless there were serious outliers present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Optimising Fourth Quadrant Error\n",
    "Now lets imagine a scenario where we don't have equal preference for an under/over prediction. Lets say we really don't want to underpredict when the target variable is higher than the mean value. More formally we want to treat errors in the fourth quadrant as 100 more costly as those elsewhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourth_quadrant(predictions, actual, multiplier = 100):\n",
    "    \"\"\"\n",
    "    Calculate Mean Absolute Error\n",
    "\n",
    "    Arguments:\n",
    "        predictions: predictions numpy array of size (no_examples, no_particles)\n",
    "        actuals: 1D numpy array of size (no_examples, 1)\n",
    "        multiplier: int - how much weight to give underpredictions for positive actual values\n",
    "\n",
    "    Returns:\n",
    "        mae: mae for each particle - numpy array of size (1, no_particles)\n",
    "    \"\"\"\n",
    "    assert(predictions.shape[0] == actual.shape[0])\n",
    "\n",
    "    if len(predictions.shape) == 1:\n",
    "        predictions = predictions.reshape(-1, 1)\n",
    "    if len(actual.shape) == 1:\n",
    "        actual = actual.reshape(-1, 1)\n",
    "     \n",
    "    errors = actual - predictions\n",
    "    assert(errors.shape == predictions.shape)\n",
    " \n",
    "    negative_error_index = errors < 0\n",
    "    positive_actual_index = actual > np.mean(actual)\n",
    "\n",
    "    extra_weight_index = np.logical_and(negative_error_index, positive_actual_index)\n",
    "\n",
    "    # cautious adjustment\n",
    "    adjustment = np.ones(shape = errors.shape)\n",
    "    adjustment[extra_weight_index] = multiplier\n",
    "\n",
    "    # squared error with adjustment\n",
    "    adjusted_squared_error = np.multiply(np.square(errors), adjustment)\n",
    "\n",
    "    return np.mean(adjusted_squared_error, axis = 0, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Particle Swarm Regression using fourth_quadrant\n",
    "PSO_4Q = LinearRegression()\n",
    "PSO_4Q.fit(X_train, log_Y_train, optimiser = 'PSO', loss = fourth_quadrant, num_iterations = 5000, no_particles = 500)\n",
    "\n",
    "# Compare model test error using fourth_quadrant\n",
    "print(f\"OLS has a test fourth_quadrant error of: {evaluate(OLS, X_test, log_Y_test, fourth_quadrant)}\")\n",
    "print(f\"PSO has a test fourth_quadrant error of: {evaluate(PSO_4Q, X_test, log_Y_test, fourth_quadrant)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSO Model trained on fourth quadrant\n",
    "residual_scatter_plot(predicted = PSO_4Q.predict(X_train),\n",
    "                      actual    = log_Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that compared to the chart below, the model trained on fourth quadrant error makes less errors in the fourth quadrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinary Least Squares\n",
    "residual_scatter_plot(predicted = OLS.predict(X_train),\n",
    "                      actual    = log_Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is this significant?\n",
    "The boston housing dataset is quite small with only 519 data points. We would like a methodology for assessing the significance of our findings. Our first thought might be to do 10-fold cross validation, and compare the performance between both algorithms on each fold using a students paired t test. However this would be incorrect, since one of the core assumptions (independence) of the t test would be violated. \n",
    "\n",
    "[Machine Learning Mastery](https://machinelearningmastery.com/statistical-significance-tests-for-comparing-machine-learning-algorithms/) explain more about the pitfalls of performing significance testing on cross validaiton data. The author introduces us to Perhaps the seminal work on this topic -  the 1998 paper titled “Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms” by Thomas Dietterich.[Dietterich (1998)](https://sci2s.ugr.es/keel/pdf/algorithm/articulo/dietterich1998.pdf)\n",
    "\n",
    "Dietterich recommends a resampling method of his own devising called 5×2 cross-validation that involves 5 repeats of 2-fold cross-validation. Two folds are chosen to ensure that each observation appears only in the train or test dataset for a single estimate of model skill. A paired Student’s t-test is used on the results, updated to better reflect the limited degrees of freedom given the dependence between the estimated skill scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(no_repeats, no_folds, loss, kwargs):\n",
    "    \"\"\"\n",
    "    Approach:\n",
    "        We want the random numbers for repeat 1 to be different for repeat 2, etc\n",
    "        however we also want the random numbers for repeat 1, to be the same\n",
    "        each time we run the cross_validation to ensure we can compare algorithm\n",
    "        performance. We therefore set the seed to be the repeat number.\n",
    "    \"\"\"  \n",
    "    boston = load_boston()\n",
    "\n",
    "    X = np.array(boston.data)\n",
    "    Y = np.log(np.array(boston.target))\n",
    "\n",
    "    no_examples, no_features = X.shape\n",
    "\n",
    "    cv_results = {}\n",
    "\n",
    "    for repeat in range(NO_REPEATS):\n",
    "        print(f\"repeat: {repeat}\")\n",
    "        \n",
    "        np.random.seed(repeat)\n",
    "        folds = np.random.randint(low = 0, high = NO_FOLDS , size = no_examples)\n",
    "        \n",
    "        cv_results[repeat] = {}\n",
    "  \n",
    "        for fold in range(NO_FOLDS):\n",
    "            print(f\"fold: {fold}\")\n",
    "\n",
    "            X_train = X[folds != fold,:]\n",
    "            X_test  = X[folds == fold,:]\n",
    "            Y_train = Y[folds != fold]\n",
    "            Y_test  = Y[folds == fold]           \n",
    "\n",
    "            # train model\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, Y_train, **kwargs)\n",
    "\n",
    "            # evaluate model\n",
    "            cv_results[repeat][fold] = evaluate(model, X_test, Y_test, loss)\n",
    "\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_LOSS = Mae\n",
    "\n",
    "print(\"\\t Training OLS Model\")\n",
    "ols_results_mae = cross_validate(no_repeats = NO_REPEATS, no_folds = NO_FOLDS, loss = CUSTOM_LOSS, \n",
    "                             kwargs = {\"optimiser\":\"OLS\"})\n",
    "\n",
    "print(\"\\t Training PSO Model\")\n",
    "pso_results_mae = cross_validate(no_repeats = NO_REPEATS, no_folds = NO_FOLDS, loss = CUSTOM_LOSS,\n",
    "                             kwargs = {\"optimiser\":'PSO', \"loss\":CUSTOM_LOSS, \"num_iterations\":5000, \"no_particles\":500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparing Models using Mean Absolute Error\")\n",
    "for repeat in range(NO_REPEATS):\n",
    "    for fold in range(NO_FOLDS):\n",
    "        print(f\"Error for repeat {repeat} fold {fold}: OLS: {np.round(ols_results_mae[repeat][fold],4)} PSO: {np.round(pso_results_mae[repeat][fold],4)} difference : {np.round(ols_results_mae[repeat][fold] - pso_results_mae[repeat][fold],4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twlearn.statistics import five_by_two_cv, p_value\n",
    "\n",
    "t_statistic, average_differences = five_by_two_cv(ols_results_mae, pso_results_mae)\n",
    "\n",
    "for repeat in average_differences:\n",
    "    print(f\"Average difference for repeat {repeat}: {np.round(average_differences[repeat],4)}\")\n",
    "\n",
    "p = p_value(t_statistic = t_statistic,\n",
    "            degrees_freedom = 5,\n",
    "            sided = 2)\n",
    "print(f\"\\n The t statistic is {np.round(t_statistic,2)} which has a p value of {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value associate with this test is not significant at the 5% level, so we find no evidence to reject the null hypothesis that there is no difference between the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Fourth Quadrant Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the distribution of error vs actual values. What if we really didn't want to underpredict at the high actual values? <insert image>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_LOSS = fourth_quadrant\n",
    "\n",
    "print(\"Training OLS Model\")\n",
    "ols_results_4Q = cross_validate(no_repeats = NO_REPEATS, no_folds = NO_FOLDS, loss = CUSTOM_LOSS, \n",
    "                             kwargs = {\"optimiser\":\"OLS\"})\n",
    "\n",
    "print(\"Training PSO Model\")\n",
    "pso_results_4Q = cross_validate(no_repeats = NO_REPEATS, no_folds = NO_FOLDS, loss = CUSTOM_LOSS,\n",
    "                             kwargs = {\"optimiser\":'PSO', \"loss\":CUSTOM_LOSS, \"num_iterations\":5000, \"no_particles\":500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparing Models using Fourth Quadrant Error\")\n",
    "for repeat in range(NO_REPEATS):\n",
    "    for fold in range(NO_FOLDS):\n",
    "        print(f\"Error for repeat {repeat} fold {fold}: OLS: {np.round(ols_results_4Q[repeat][fold],2)} PSO: {np.round(pso_results_4Q[repeat][fold],2)} difference : {np.round(ols_results_4Q[repeat][fold] - pso_results_4Q[repeat][fold],2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_statistic, average_differences = five_by_two_cv(ols_results_4Q, pso_results_4Q)\n",
    "\n",
    "for repeat in average_differences:\n",
    "    print(f\"Average difference for repeat {repeat}: {np.round(average_differences[repeat],4)}\")\n",
    "\n",
    "p = p_value(t_statistic = t_statistic,\n",
    "            degrees_freedom = 5,\n",
    "            sided = 2)\n",
    "\n",
    "print(f\"\\n The t statistic is {np.round(t_statistic,2)} which has a p value of {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p value associated with this test is 0.056 which is in the grey area of betwen 5% and 10% significance level. Therefore we conclude that there is weak evidence that the model trained on fourth quadrant error performs better than ordinary least squares with respect to fourth quadrant error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
